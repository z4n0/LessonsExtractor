{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11UfY8Zto4MKM2nZF8xSbiblUOBtD7j9Q",
      "authorship_tag": "ABX9TyPewRuEoP7sW3ZIFMvn958e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z4n0/LessonsExtractor/blob/main/lessonsExtractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0kO_tqcov-aI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08a4037-9b2d-4d18-a3b7-2d6598f44288"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls /content/drive/MyDrive/lessonsExtractor"
      ],
      "metadata": {
        "id": "hl37iVQ3wAM7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1ae414-f9c5-4323-dda2-896f8be1d7be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "converted_video.mp4  CSR-20221124.mp4  lessonsExtractor.ipynb  \u001b[0m\u001b[01;34mslides\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NB: TURN ON THE T4 GPU"
      ],
      "metadata": {
        "id": "tGkXrIYPFgQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "e-DxNH_1u1Wt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06636928-90e0-4b4c-e1a3-ce259af12415"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/lessonsExtractor/"
      ],
      "metadata": {
        "id": "4ccQmw_FyoBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb68373-afb7-4af9-d7b2-9857e84ec0fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/lessonsExtractor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "eQo9mJvpypKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conversion from mkv to mp4"
      ],
      "metadata": {
        "id": "DJW17lm8zjOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i prova.mkv"
      ],
      "metadata": {
        "id": "vAakNFvfAiKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ffmpeg -i prova.mkv -c:v libx264 -c:a aac prova_converted.mp4"
      ],
      "metadata": {
        "id": "H7VWtpUyzhah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "reduce quality and bit rate to help the code run faster"
      ],
      "metadata": {
        "id": "69BFKqis0FAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -i prova.mkv -s 1280x720 -b:v 1M -c:v libx264 -c:a aac prova_converted.mp4"
      ],
      "metadata": {
        "id": "s1ULV6OC0EG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CODE TO REDUCE QUALITY AND BIT RATE AND SELECT A PIECE OF THE MP4 VIDEO"
      ],
      "metadata": {
        "id": "lEh4fvV54lY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#check if ffmpeg is compatible for hw accelleration\n",
        "\n",
        "TURN ON THE T4 GPU!!\n",
        "\n",
        "A typical successful output snippet might start like this:\n"
      ],
      "metadata": {
        "id": "QVoSM5g2EfL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Encoder h264_nvenc [NVIDIA NVENC H.264 encoder]:\n",
        "    General capabilities: delay hardware\n",
        "    Threading capabilities: none\n",
        "    Supported pixel formats: nv12 p010le yuv444p nv16 yuv444p16le bgr0 rgb0 ...\n",
        "    ...\n"
      ],
      "metadata": {
        "id": "3GzWKPyaEn2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code to check if you can use accelleration\n",
        "!ffmpeg -h encoder=h264_nvenc"
      ],
      "metadata": {
        "id": "xf6mphRYEII0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if the tpu is on\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoSA9mYqF9uS",
        "outputId": "2a2f851a-5a50-4fde-e77e-c82588c909b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 16 17:06:47 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZvvJtPGlBLP",
        "outputId": "ea758a21-6041-4f2c-a01a-75fe4d4d6e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "converted_video.mp4  CSR-20221124.mp4  lessonsExtractor.ipynb  \u001b[0m\u001b[01;34mslides\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_video_path = '9_Semantic_Segmentation.mp4'\n",
        "\n",
        "# Extract base name and remove extension to form the new output file name\n",
        "base_name = os.path.basename(input_video_path)\n",
        "file_name_without_ext = os.path.splitext(base_name)[0]\n",
        "output_video_path = f\"/content/drive/MyDrive/lessonsExtractor/{file_name_without_ext}_converted.mp4\"\n",
        "\n",
        "resolution = \"1280x720\"\n",
        "bitrate = \"1M\"\n",
        "start_time = \"00:40:00\"  # Start at 40 minutes\n",
        "#duration = \"00:30:00\"  # Duration of 50 minutes, will stop at the end of the video if it's shorter\n",
        "\n",
        "# Standard FFmpeg command\n",
        "#!ffmpeg -ss {start_time} -i {input_video_path} -t {duration} -s {resolution} -b:v {bitrate} -c:v libx264 -c:a aac {output_video_path}\n",
        "\n",
        "# GPU accelerated version\n",
        "!ffmpeg -hwaccel cuda -ss {start_time} -i {input_video_path} -s {resolution} -b:v {bitrate} -c:v h264_nvenc -c:a aac {output_video_path}\n"
      ],
      "metadata": {
        "id": "7WLs5IdG4k7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41085b6f-3189-4f46-d5f7-b4160ec12263"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '9_Semantic_Segmentation.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : iso6\n",
            "    minor_version   : 1\n",
            "    compatible_brands: iso6dsmsmsixdash\n",
            "    creation_time   : 2023-11-13T09:35:34.000000Z\n",
            "    encoder         : GPAC-2.3-DEV-rev566-g50c2ab06f-master\n",
            "  Duration: 01:42:06.11, start: 0.048000, bitrate: 467 kb/s\n",
            "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 1920x1080 [SAR 1:1 DAR 16:9], 0 kb/s, 25 fps, 25 tbr, 12800 tbn, 50 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 32000 Hz, mono, fltp, 0 kb/s (default)\n",
            "    Metadata:\n",
            "      handler_name    : SoundHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (h264_nvenc))\n",
            "  Stream #0:1 -> #0:1 (aac (native) -> aac (native))\n",
            "Press [q] to stop, [?] for help\n",
            "Output #0, mp4, to '/content/drive/MyDrive/lessonsExtractor/9_Semantic_Segmentation_converted.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : iso6\n",
            "    minor_version   : 1\n",
            "    compatible_brands: iso6dsmsmsixdash\n",
            "    encoder         : Lavf58.76.100\n",
            "  Stream #0:0(und): Video: h264 (Main) (avc1 / 0x31637661), nv12(tv, progressive), 1280x720 [SAR 1:1 DAR 16:9], q=2-31, 1000 kb/s, 25 fps, 12800 tbn (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 h264_nvenc\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/1000000 buffer size: 2000000 vbv_delay: N/A\n",
            "  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 32000 Hz, mono, fltp, 69 kb/s (default)\n",
            "    Metadata:\n",
            "      handler_name    : SoundHandler\n",
            "      vendor_id       : [0][0][0][0]\n",
            "      encoder         : Lavc58.134.100 aac\n",
            "\u001b[0;33mMore than 1000 frames duplicated\n",
            "frame=93151 fps=667 q=10.0 Lsize=  257068kB time=01:02:05.88 bitrate= 565.2kbits/s dup=88464 drop=0 speed=26.7x    \n",
            "video:254661kB audio:1587kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.320171%\n",
            "\u001b[1;36m[aac @ 0x55b275407000] \u001b[0mQavg: 143.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "you should see the new converted_video.mp4 here. let's check!"
      ],
      "metadata": {
        "id": "lPhc983z6MQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9wwJVTh02fr",
        "outputId": "d5249750-365d-4fc9-d4ab-446c2fda58e0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9_Semantic_Segmentation_converted.mp4  converted_video.mp4  lessonsExtractor.ipynb  \u001b[0m\u001b[01;34mslides\u001b[0m/\n",
            "9_Semantic_Segmentation.mp4            CSR-20221124.mp4     output.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "55Uc9ADC06vI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_frame(frame, timestamp, output_folder='slides'):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    filename = f\"{output_folder}/slide_{timestamp}.jpg\"\n",
        "    cv2.imwrite(filename, frame)\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "def frame_difference(prev_frame, curr_frame):\n",
        "    # Compute the absolute difference between the current frame and the previous frame\n",
        "    diff = cv2.absdiff(prev_frame, curr_frame)\n",
        "    non_zero_count = np.count_nonzero(diff)\n",
        "    return non_zero_count\n",
        "\n",
        "def milliseconds_to_minutes_seconds(milliseconds):\n",
        "    minutes = int(milliseconds / 60000)\n",
        "    seconds = int((milliseconds % 60000) / 1000)\n",
        "    return f\"{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "def detect_slides(video_path, threshold=265000, skip_frames=50):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    ret, prev_frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to read the video\")\n",
        "        return []\n",
        "\n",
        "    prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame_count = 0\n",
        "    slide_changes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, curr_frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % skip_frames == 0:\n",
        "            gray_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "            if frame_difference(prev_frame, gray_frame) > threshold:\n",
        "                timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)  # Get the timestamp in milliseconds\n",
        "                timestamp_readable = milliseconds_to_minutes_seconds(timestamp_ms)\n",
        "                slide_changes.append(timestamp_readable)\n",
        "                save_frame(curr_frame, timestamp_readable)\n",
        "                prev_frame = gray_frame\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return slide_changes\n",
        "\n",
        "input_video_path = output_video_path\n",
        "slide_changes = detect_slides(input_video_path)\n",
        "print(f\"Slide changes detected at: {slide_changes}\")"
      ],
      "metadata": {
        "id": "3AjMT6gtu_4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "d4d7152d-c615-4208-e31f-2de81c4d79e3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: slides/slide_00:04.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-ba40fab60727>\u001b[0m in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0minput_video_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_video_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mslide_changes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_slides\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_video_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Slide changes detected at: {slide_changes}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-ba40fab60727>\u001b[0m in \u001b[0;36mdetect_slides\u001b[0;34m(video_path, threshold, skip_frames)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"detected slides {len(slide_changes)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyUQrIWK1Tpc",
        "outputId": "7bba1bcb-728d-4803-a71d-02ebbb1bb8ed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "detected slides 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio extraction"
      ],
      "metadata": {
        "id": "NNAVDti0a1Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "4tHPDl3Wa36U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7976ab15-b88a-4a0f-8555-349623dae038"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-2yfuv40l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-2yfuv40l\n",
            "  Resolved https://github.com/openai/whisper.git to commit e58f28804528831904c3b6f2c0e473f346223433\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=1fa3e8740a77a4a8630e2cda3a6059a90a750146750ebcb293c83243c39d227d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0hntfi17/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "def transcribe_video(video_path, model_size='base'):\n",
        "    model = whisper.load_model(model_size)\n",
        "    result = model.transcribe(video_path)\n",
        "    return result"
      ],
      "metadata": {
        "id": "0fIvPrh4a8KU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interleave_transcription_with_slides(transcription, slide_changes):\n",
        "    interleaved_data = []\n",
        "    current_slide = 0\n",
        "\n",
        "    for segment in transcription['segments']:\n",
        "        # Find the next slide change that occurs after this segment starts\n",
        "        while current_slide < len(slide_changes) and slide_changes[current_slide] <= segment['start']:\n",
        "            interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "            current_slide += 1\n",
        "\n",
        "        # Add the transcribed text\n",
        "        interleaved_data.append({'type': 'transcription', 'timestamp': segment['start'], 'text': segment['text']})\n",
        "\n",
        "    # Handle any remaining slide changes\n",
        "    while current_slide < len(slide_changes):\n",
        "        interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "        current_slide += 1\n",
        "\n",
        "    return interleaved_data"
      ],
      "metadata": {
        "id": "cw0oBthdbBJo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pdf or docx creation"
      ],
      "metadata": {
        "id": "bqXpP6KDbPIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMh1C3O0bjHn",
        "outputId": "30508f85-c9a3-4c20-805b-94c1b834da72"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "\n",
        "from docx.shared import Inches\n",
        "\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "\n",
        "def create_document(interleaved_data, video_path, slides_folder='slides'):\n",
        "    # Extract base name and remove extension to form the DOCX file name\n",
        "    base_name = os.path.basename(video_path)\n",
        "    file_name_without_ext = os.path.splitext(base_name)[0]\n",
        "    output_file = f'{file_name_without_ext}.docx'\n",
        "\n",
        "    doc = Document()\n",
        "    for item in interleaved_data:\n",
        "        if item['type'] == 'slide_change':\n",
        "            # Add slide image\n",
        "            slide_image_path = f\"{slides_folder}/slide_{item['timestamp']}.jpg\"\n",
        "            p = doc.add_paragraph()\n",
        "            try:\n",
        "                p.add_run().add_picture(slide_image_path, width=Inches(6))  # Adjust the width as needed\n",
        "            except Exception as e:\n",
        "                p.add_run(f\"Could not load slide image: {slide_image_path}\")\n",
        "        else:\n",
        "            # Add transcribed text\n",
        "            p = doc.add_paragraph(item['text'])\n",
        "\n",
        "    doc.save(output_file)\n",
        "    print(f\"Document saved as: {output_file}\")\n",
        "\n",
        "\n",
        "def convert_timestamp_to_seconds(timestamp_str):\n",
        "    minutes, seconds = map(int, timestamp_str.split(':'))\n",
        "    return minutes * 60 + seconds\n",
        "\n",
        "def interleave_transcription_with_slides(transcription, slide_changes):\n",
        "    # Convert slide change timestamps to seconds\n",
        "    slide_changes_seconds = [convert_timestamp_to_seconds(ts) for ts in slide_changes]\n",
        "\n",
        "    interleaved_data = []\n",
        "    current_slide = 0\n",
        "\n",
        "    for segment in transcription['segments']:\n",
        "        # Find the next slide change that occurs after this segment starts\n",
        "        while current_slide < len(slide_changes_seconds) and slide_changes_seconds[current_slide] <= segment['start']:\n",
        "            interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "            current_slide += 1\n",
        "\n",
        "        # Add the transcribed text\n",
        "        interleaved_data.append({'type': 'transcription', 'timestamp': segment['start'], 'text': segment['text']})\n",
        "\n",
        "    # Handle any remaining slide changes\n",
        "    while current_slide < len(slide_changes_seconds):\n",
        "        interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "        current_slide += 1\n",
        "\n",
        "    return interleaved_data\n",
        "\n",
        "# Transcribe video\n",
        "transcription = transcribe_video(output_video_path)\n",
        "\n",
        "# Interleave transcription with slide changes\n",
        "interleaved_data = interleave_transcription_with_slides(transcription, slide_changes)\n",
        "\n",
        "# Create DOCX document\n",
        "create_document(interleaved_data, output_video_path)"
      ],
      "metadata": {
        "id": "Z1sK46IiboN8"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}