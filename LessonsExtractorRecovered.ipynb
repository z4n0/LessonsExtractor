{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/z4n0/LessonsExtractor/blob/main/LessonsExtractorRecovered.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kO_tqcov-aI",
        "outputId": "fc046225-7b42-4858-8b7d-1176e38567bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl37iVQ3wAM7",
        "outputId": "39e01ce5-dcbf-4abf-fc91-d660a193d3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;36m/content/drive/MyDrive/lessonsExtractor\u001b[0m@\n"
          ]
        }
      ],
      "source": [
        "%ls /content/drive/MyDrive/lessonsExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGkXrIYPFgQ3"
      },
      "source": [
        "NB: TURN ON THE T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-DxNH_1u1Wt",
        "outputId": "87575224-44c7-43fe-9285-f8547d8e875d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ccQmw_FyoBn",
        "outputId": "75c0edd2-22c4-49ff-ea40-e0b638c13714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1bQb8gi6zCwEeUgzwHQLPETmzfh9dMdVq/lessonsExtractor\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/lessonsExtractor/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQo9mJvpypKv",
        "outputId": "ab2cd484-c2b8-4755-d019-906964c9bc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 20240404_converted.docx       lez15_converted.docx\n",
            " gpu-20240430_converted.docx   lez15.mp4\n",
            " gpu-20240430.mp4              lez3_converted.docx\n",
            " gpu-20240515_converted.docx   lez3_slides_only.docx\n",
            " gpu-20240515.mp4              lez6.mp4\n",
            " gpu-20240517.mp4              lez9.docx\n",
            " gpu-20240529_converted.docx  'Marcello Restelli_s Personal Room-20240312 1333-1_converted.docx'\n",
            " gpu-20240529.mp4             'Marcello Restelli_s Personal Room-20240326 1330-1_converted.docx'\n",
            " gpu-20240531.mp4              ML20240416_converted.docx\n",
            " LessonsExtractor.ipynb        ML20240430_converted.docx\n",
            " lez13_converted.docx          ML20240502_converted.docx\n",
            " lez13.mp4\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRxLyAGhTmC"
      },
      "source": [
        "#slide extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdA19uZ0h6Ja"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2tMEmLbGOof",
        "outputId": "4d0da7cf-2b2b-467c-9884-c0693c833cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 20240404_converted.docx       lez15_converted.docx\n",
            " gpu-20240430_converted.docx   lez15.mp4\n",
            " gpu-20240430.mp4              lez3_converted.docx\n",
            " gpu-20240515_converted.docx   lez3_slides_only.docx\n",
            " gpu-20240515.mp4              lez6.mp4\n",
            " gpu-20240517.mp4              lez9.docx\n",
            " gpu-20240529_converted.docx  'Marcello Restelli_s Personal Room-20240312 1333-1_converted.docx'\n",
            " gpu-20240529.mp4             'Marcello Restelli_s Personal Room-20240326 1330-1_converted.docx'\n",
            " gpu-20240531.mp4              ML20240416_converted.docx\n",
            " LessonsExtractor.ipynb        ML20240430_converted.docx\n",
            " lez13_converted.docx          ML20240502_converted.docx\n",
            " lez13.mp4\n"
          ]
        }
      ],
      "source": [
        "%ls #to copy paste the name of the video you want to transcribe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYbrd1GWKio4",
        "outputId": "8318f4ba-738f-4be6-e538-c82735430cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1bQb8gi6zCwEeUgzwHQLPETmzfh9dMdVq/lessonsExtractor\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/lessonsExtractor/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WLs5IdG4k7P"
      },
      "outputs": [],
      "source": [
        "input_video_path = 'gpu-20240515.mp4'\n",
        "\n",
        "# Extract base name and remove extension to form the new output file name\n",
        "base_name = os.path.basename(input_video_path)\n",
        "file_name_without_ext = os.path.splitext(base_name)[0]\n",
        "output_video_path = f\"/content/drive/MyDrive/lessonsExtractor/{file_name_without_ext}_converted.mp4\"\n",
        "resolution = \"1280x720\"\n",
        "bitrate = \"1M\"\n",
        "start_time = \"00:00:00\"  # Start at 0 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnEQxoQR4O80"
      },
      "outputs": [],
      "source": [
        "#duration = \"00:30:00\"  # Duration of 50 minutes, will stop at the end of the video if it's shorter\n",
        "\n",
        "# Standard FFmpeg command\n",
        "#!ffmpeg -ss {start_time} -i {input_video_path} -t {duration} -s {resolution} -b:v {bitrate} -c:v libx264 -c:a aac {output_video_path}\n",
        "\n",
        "# GPU accelerated version\n",
        "#!ffmpeg -hwaccel cuda -ss {start_time} -i {input_video_path} -s {resolution} -b:v {bitrate} -c:v h264_nvenc -c:a aac {output_video_path}\n",
        "#!ffmpeg -i {input_video_path} -ss {start_time} -s {resolution} -b:v {bitrate} -c:v h264_nvenc -c:a aac {output_video_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPhc983z6MQw"
      },
      "source": [
        "you should see the new converted_video.mp4 here. let's check!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Gdu8QL7bQB"
      },
      "source": [
        "**NB: THE PARAMETER THRESHOLD IN DETECT_SLIDES(....) HAS TO BE FINE TUNED FOR EACH VIDEO**\n",
        "\n",
        "the higher the value the more selective it becomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5qJZeW7RJE7"
      },
      "outputs": [],
      "source": [
        "def sum_time(time1, time2):\n",
        "    # Convert time to seconds\n",
        "    def time_to_seconds(time_str):\n",
        "        h, m, s = map(int, time_str.split(':'))\n",
        "        return h * 3600 + m * 60 + s\n",
        "\n",
        "    # Convert seconds back to time\n",
        "    def seconds_to_time(seconds):\n",
        "        h = seconds // 3600\n",
        "        m = (seconds % 3600) // 60\n",
        "        s = seconds % 60\n",
        "        return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "    total_seconds = time_to_seconds(time1) + time_to_seconds(time2)\n",
        "    return seconds_to_time(total_seconds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "collapsed": true,
        "id": "3AjMT6gtu_4j",
        "outputId": "f3cd4915-154f-406c-f59b-dc9c04277def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00:00:01\n",
            "Saved: slides/slide_00:00:01.jpg\n",
            "00:00:02\n",
            "Saved: slides/slide_00:00:02.jpg\n",
            "00:00:03\n",
            "Saved: slides/slide_00:00:03.jpg\n",
            "00:00:04\n",
            "Saved: slides/slide_00:00:04.jpg\n",
            "00:00:06\n",
            "Saved: slides/slide_00:00:06.jpg\n",
            "00:00:07\n",
            "Saved: slides/slide_00:00:07.jpg\n",
            "00:00:08\n",
            "Saved: slides/slide_00:00:08.jpg\n",
            "00:00:09\n",
            "Saved: slides/slide_00:00:09.jpg\n",
            "00:00:10\n",
            "Saved: slides/slide_00:00:10.jpg\n",
            "00:00:12\n",
            "Saved: slides/slide_00:00:12.jpg\n",
            "00:00:13\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e4e826392579>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#input_video_path = output_video_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mslide_changes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_slides\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_video_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Slide changes detected at: {slide_changes}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e4e826392579>\u001b[0m in \u001b[0;36mdetect_slides\u001b[0;34m(video_path, threshold, skip_frames)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0madjusted_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestamp_readable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust the timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mslide_changes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjusted_timestamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0msave_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_timestamp\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use adjusted timestamp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0mprev_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgray_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mframe_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e4e826392579>\u001b[0m in \u001b[0;36msave_frame\u001b[0;34m(frame, timestamp, output_folder)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{output_folder}/slide_{timestamp}.jpg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved: {filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def save_frame(frame, timestamp, output_folder='slides'):\n",
        "    print(sum_time(timestamp, start_time)) ## WHERE sum_time IS DEFINED?\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    filename = f\"{output_folder}/slide_{timestamp}.jpg\"\n",
        "    cv2.imwrite(filename, frame)\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "def frame_difference(prev_frame, curr_frame):\n",
        "    # Compute the absolute difference between the current frame and the previous frame\n",
        "    diff = cv2.absdiff(prev_frame, curr_frame)\n",
        "    non_zero_count = np.count_nonzero(diff)\n",
        "    return non_zero_count\n",
        "\n",
        "def milliseconds_to_hh_mm_ss(milliseconds):\n",
        "    hours = int(milliseconds / 3600000)\n",
        "    minutes = int((milliseconds % 3600000) / 60000)\n",
        "    seconds = int((milliseconds % 60000) / 1000)\n",
        "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "#trashold is to be fine tuned for each video\n",
        "def detect_slides(video_path, threshold=420000, skip_frames=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    ret, prev_frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to read the video\")\n",
        "        return []\n",
        "\n",
        "    prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame_count = 0\n",
        "    slide_changes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, curr_frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % skip_frames == 0:\n",
        "            gray_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "            if frame_difference(prev_frame, gray_frame) > threshold:\n",
        "                timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "                timestamp_readable = milliseconds_to_hh_mm_ss(timestamp_ms)\n",
        "                adjusted_timestamp = sum_time(timestamp_readable, start_time)  # Adjust the timestamp\n",
        "                slide_changes.append(adjusted_timestamp)\n",
        "                save_frame(curr_frame, adjusted_timestamp)  # Use adjusted timestamp\n",
        "                prev_frame = gray_frame\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return slide_changes\n",
        "\n",
        "#input_video_path = output_video_path\n",
        "slide_changes = detect_slides(input_video_path)\n",
        "print(f\"Slide changes detected at: {slide_changes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCgnJHZKZwh1"
      },
      "source": [
        "##NEW OPTIMIZED FUNCTION BY LessonExtractorAI (IT SEEMS TO WORK VERY WELL)\n",
        "Instead of just using frame difference, you could incorporate detection based on more stable features, such as outlines or text in slides. This will require the use of computer vision libraries such as OpenCV. The idea is to identify unique characteristics in each slide and detect when these change significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AX7JYMLYev1"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def find_key_features(image):\n",
        "    # Verifica il numero di canali dell'immagine\n",
        "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
        "        # L'immagine è a colori, convertila in scala di grigi\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        # L'immagine è già in scala di grigi\n",
        "        gray = image\n",
        "\n",
        "    # Applica il rilevamento dei bordi\n",
        "    edges = cv2.Canny(gray, 50, 150)\n",
        "    # Trova i contorni\n",
        "    contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    return contours\n",
        "\n",
        "\n",
        "def compare_slides(slide1, slide2, threshold=0.7):\n",
        "    # Compare the key features of two slides\n",
        "    slide1_features = find_key_features(slide1)\n",
        "    slide2_features = find_key_features(slide2)\n",
        "\n",
        "    # Verifica che almeno uno dei frame abbia dei contorni rilevati\n",
        "    max_len_features = max(len(slide1_features), len(slide2_features))\n",
        "    if max_len_features == 0:\n",
        "        return False  # Entrambi i frame non hanno contorni, quindi non confrontarli\n",
        "\n",
        "    # Example comparison logic: Compare the number of contours\n",
        "    if abs(len(slide1_features) - len(slide2_features)) / max_len_features > threshold:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def save_frame(frame, timestamp, output_folder='slides'):\n",
        "    print(sum_time(timestamp, start_time)) ## WHERE sum_time IS DEFINED?\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    filename = f\"{output_folder}/slide_{timestamp}.jpg\"\n",
        "    cv2.imwrite(filename, frame)\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "def milliseconds_to_hh_mm_ss(milliseconds):\n",
        "    hours = int(milliseconds / 3600000)\n",
        "    minutes = int((milliseconds % 3600000) / 60000)\n",
        "    seconds = int((milliseconds % 60000) / 1000)\n",
        "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sYFx6EcgC70",
        "outputId": "c4cffa50-d6f8-456d-ed7b-455d470cb7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "00:00:01\n",
            "Saved: slides/slide_00:00:01.jpg\n",
            "Slide change detected at: 00:00:01\n",
            "00:00:02\n",
            "Saved: slides/slide_00:00:02.jpg\n",
            "Slide change detected at: 00:00:02\n",
            "00:00:04\n",
            "Saved: slides/slide_00:00:04.jpg\n",
            "Slide change detected at: 00:00:04\n",
            "00:00:06\n",
            "Saved: slides/slide_00:00:06.jpg\n",
            "Slide change detected at: 00:00:06\n",
            "00:00:09\n",
            "Saved: slides/slide_00:00:09.jpg\n",
            "Slide change detected at: 00:00:09\n",
            "00:00:10\n",
            "Saved: slides/slide_00:00:10.jpg\n",
            "Slide change detected at: 00:00:10\n",
            "00:00:15\n",
            "Saved: slides/slide_00:00:15.jpg\n",
            "Slide change detected at: 00:00:15\n",
            "00:00:26\n",
            "Saved: slides/slide_00:00:26.jpg\n",
            "Slide change detected at: 00:00:26\n",
            "00:00:27\n",
            "Saved: slides/slide_00:00:27.jpg\n",
            "Slide change detected at: 00:00:27\n",
            "00:00:28\n",
            "Saved: slides/slide_00:00:28.jpg\n",
            "Slide change detected at: 00:00:28\n",
            "00:02:33\n",
            "Saved: slides/slide_00:02:33.jpg\n",
            "Slide change detected at: 00:02:33\n",
            "00:02:36\n",
            "Saved: slides/slide_00:02:36.jpg\n",
            "Slide change detected at: 00:02:36\n",
            "00:02:39\n",
            "Saved: slides/slide_00:02:39.jpg\n",
            "Slide change detected at: 00:02:39\n",
            "00:03:13\n",
            "Saved: slides/slide_00:03:13.jpg\n",
            "Slide change detected at: 00:03:13\n",
            "00:06:40\n",
            "Saved: slides/slide_00:06:40.jpg\n",
            "Slide change detected at: 00:06:40\n",
            "00:11:00\n",
            "Saved: slides/slide_00:11:00.jpg\n",
            "Slide change detected at: 00:11:00\n",
            "00:11:40\n",
            "Saved: slides/slide_00:11:40.jpg\n",
            "Slide change detected at: 00:11:40\n",
            "00:12:08\n",
            "Saved: slides/slide_00:12:08.jpg\n",
            "Slide change detected at: 00:12:08\n",
            "00:13:16\n",
            "Saved: slides/slide_00:13:16.jpg\n",
            "Slide change detected at: 00:13:16\n",
            "00:14:42\n",
            "Saved: slides/slide_00:14:42.jpg\n",
            "Slide change detected at: 00:14:42\n",
            "00:18:06\n",
            "Saved: slides/slide_00:18:06.jpg\n",
            "Slide change detected at: 00:18:06\n",
            "00:19:51\n",
            "Saved: slides/slide_00:19:51.jpg\n",
            "Slide change detected at: 00:19:51\n",
            "00:21:26\n",
            "Saved: slides/slide_00:21:26.jpg\n",
            "Slide change detected at: 00:21:26\n",
            "00:24:40\n",
            "Saved: slides/slide_00:24:40.jpg\n",
            "Slide change detected at: 00:24:40\n",
            "00:26:40\n",
            "Saved: slides/slide_00:26:40.jpg\n",
            "Slide change detected at: 00:26:40\n",
            "00:28:04\n",
            "Saved: slides/slide_00:28:04.jpg\n",
            "Slide change detected at: 00:28:04\n",
            "00:29:09\n",
            "Saved: slides/slide_00:29:09.jpg\n",
            "Slide change detected at: 00:29:09\n",
            "00:29:16\n",
            "Saved: slides/slide_00:29:16.jpg\n",
            "Slide change detected at: 00:29:16\n",
            "00:29:54\n",
            "Saved: slides/slide_00:29:54.jpg\n",
            "Slide change detected at: 00:29:54\n",
            "00:34:34\n",
            "Saved: slides/slide_00:34:34.jpg\n",
            "Slide change detected at: 00:34:34\n",
            "00:34:58\n",
            "Saved: slides/slide_00:34:58.jpg\n",
            "Slide change detected at: 00:34:58\n",
            "00:36:03\n",
            "Saved: slides/slide_00:36:03.jpg\n",
            "Slide change detected at: 00:36:03\n",
            "00:37:54\n",
            "Saved: slides/slide_00:37:54.jpg\n",
            "Slide change detected at: 00:37:54\n",
            "00:37:55\n",
            "Saved: slides/slide_00:37:55.jpg\n",
            "Slide change detected at: 00:37:55\n",
            "00:37:56\n",
            "Saved: slides/slide_00:37:56.jpg\n",
            "Slide change detected at: 00:37:56\n",
            "00:37:58\n",
            "Saved: slides/slide_00:37:58.jpg\n",
            "Slide change detected at: 00:37:58\n",
            "00:38:01\n",
            "Saved: slides/slide_00:38:01.jpg\n",
            "Slide change detected at: 00:38:01\n",
            "00:39:13\n",
            "Saved: slides/slide_00:39:13.jpg\n",
            "Slide change detected at: 00:39:13\n",
            "00:43:42\n",
            "Saved: slides/slide_00:43:42.jpg\n",
            "Slide change detected at: 00:43:42\n",
            "00:45:40\n",
            "Saved: slides/slide_00:45:40.jpg\n",
            "Slide change detected at: 00:45:40\n",
            "00:45:42\n",
            "Saved: slides/slide_00:45:42.jpg\n",
            "Slide change detected at: 00:45:42\n",
            "00:45:45\n",
            "Saved: slides/slide_00:45:45.jpg\n",
            "Slide change detected at: 00:45:45\n",
            "00:47:06\n",
            "Saved: slides/slide_00:47:06.jpg\n",
            "Slide change detected at: 00:47:06\n",
            "00:48:54\n",
            "Saved: slides/slide_00:48:54.jpg\n",
            "Slide change detected at: 00:48:54\n",
            "00:50:22\n",
            "Saved: slides/slide_00:50:22.jpg\n",
            "Slide change detected at: 00:50:22\n",
            "00:51:58\n",
            "Saved: slides/slide_00:51:58.jpg\n",
            "Slide change detected at: 00:51:58\n",
            "00:52:03\n",
            "Saved: slides/slide_00:52:03.jpg\n",
            "Slide change detected at: 00:52:03\n",
            "00:52:04\n",
            "Saved: slides/slide_00:52:04.jpg\n",
            "Slide change detected at: 00:52:04\n",
            "00:52:10\n",
            "Saved: slides/slide_00:52:10.jpg\n",
            "Slide change detected at: 00:52:10\n",
            "00:52:16\n",
            "Saved: slides/slide_00:52:16.jpg\n",
            "Slide change detected at: 00:52:16\n",
            "00:55:01\n",
            "Saved: slides/slide_00:55:01.jpg\n",
            "Slide change detected at: 00:55:01\n",
            "00:55:14\n",
            "Saved: slides/slide_00:55:14.jpg\n",
            "Slide change detected at: 00:55:14\n",
            "00:57:20\n",
            "Saved: slides/slide_00:57:20.jpg\n",
            "Slide change detected at: 00:57:20\n",
            "00:57:22\n",
            "Saved: slides/slide_00:57:22.jpg\n",
            "Slide change detected at: 00:57:22\n",
            "00:57:26\n",
            "Saved: slides/slide_00:57:26.jpg\n",
            "Slide change detected at: 00:57:26\n",
            "00:59:15\n",
            "Saved: slides/slide_00:59:15.jpg\n",
            "Slide change detected at: 00:59:15\n",
            "01:00:31\n",
            "Saved: slides/slide_01:00:31.jpg\n",
            "Slide change detected at: 01:00:31\n",
            "01:01:32\n",
            "Saved: slides/slide_01:01:32.jpg\n",
            "Slide change detected at: 01:01:32\n",
            "01:02:00\n",
            "Saved: slides/slide_01:02:00.jpg\n",
            "Slide change detected at: 01:02:00\n",
            "01:02:22\n",
            "Saved: slides/slide_01:02:22.jpg\n",
            "Slide change detected at: 01:02:22\n",
            "01:03:03\n",
            "Saved: slides/slide_01:03:03.jpg\n",
            "Slide change detected at: 01:03:03\n",
            "01:03:27\n",
            "Saved: slides/slide_01:03:27.jpg\n",
            "Slide change detected at: 01:03:27\n",
            "01:05:02\n",
            "Saved: slides/slide_01:05:02.jpg\n",
            "Slide change detected at: 01:05:02\n",
            "01:08:03\n",
            "Saved: slides/slide_01:08:03.jpg\n",
            "Slide change detected at: 01:08:03\n",
            "01:08:04\n",
            "Saved: slides/slide_01:08:04.jpg\n",
            "Slide change detected at: 01:08:04\n",
            "01:08:06\n",
            "Saved: slides/slide_01:08:06.jpg\n",
            "Slide change detected at: 01:08:06\n",
            "01:08:07\n",
            "Saved: slides/slide_01:08:07.jpg\n",
            "Slide change detected at: 01:08:07\n",
            "01:08:08\n",
            "Saved: slides/slide_01:08:08.jpg\n",
            "Slide change detected at: 01:08:08\n",
            "01:10:54\n",
            "Saved: slides/slide_01:10:54.jpg\n",
            "Slide change detected at: 01:10:54\n",
            "01:10:55\n",
            "Saved: slides/slide_01:10:55.jpg\n",
            "Slide change detected at: 01:10:55\n",
            "01:10:56\n",
            "Saved: slides/slide_01:10:56.jpg\n",
            "Slide change detected at: 01:10:56\n",
            "01:13:06\n",
            "Saved: slides/slide_01:13:06.jpg\n",
            "Slide change detected at: 01:13:06\n",
            "01:13:31\n",
            "Saved: slides/slide_01:13:31.jpg\n",
            "Slide change detected at: 01:13:31\n",
            "01:13:32\n",
            "Saved: slides/slide_01:13:32.jpg\n",
            "Slide change detected at: 01:13:32\n",
            "01:13:33\n",
            "Saved: slides/slide_01:13:33.jpg\n",
            "Slide change detected at: 01:13:33\n"
          ]
        }
      ],
      "source": [
        "# Slides detection\n",
        "\n",
        "# Threshold value determines how different two slides must be to be considered\n",
        "# different from each other. In this specific case, the value of 0.7 is used\n",
        "# to determine the relative difference in the number of contours between two slides\n",
        "\n",
        "def detect_slides(video_path, threshold=0.05, skip_frames=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    ret, prev_frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to read the video\")\n",
        "        return []\n",
        "\n",
        "    prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame_count = 0\n",
        "    slide_changes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, curr_frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % skip_frames == 0:\n",
        "            if compare_slides(prev_frame, curr_frame, threshold):\n",
        "                timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "                timestamp_readable = milliseconds_to_hh_mm_ss(timestamp_ms)\n",
        "                slide_changes.append(timestamp_readable)\n",
        "                save_frame(curr_frame, timestamp_readable)  # Save the current slide\n",
        "                prev_frame = curr_frame\n",
        "                print(f\"Slide change detected at: {timestamp_readable}\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return slide_changes\n",
        "\n",
        "slide_changes = detect_slides(input_video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rORxnoXh6Xx"
      },
      "source": [
        "###CHECK IF IS A SLIDE (TO BE TESTED)\n",
        "text_threshold: This threshold represents the minimum amount of area in the image that must be covered by text for the image to be considered a slide. The idea is that presentation slides typically contain a significant amount of text.\n",
        "\n",
        "The value 100 in this case is an arbitrary example. If the calculated value of the text area in the image is less than text_threshold, the image is not considered a slide.\n",
        "This parameter can be adjusted based on the specific nature of the slides you are analyzing. If your slides have a lot of text, you may want to set a higher value; however, if they contain little text, a lower value may be more appropriate.\n",
        "\n",
        "feature_threshold: This threshold is used to evaluate the presence of other key features (such as specific shapes, layout, colors) that could identify an image as a slide.\n",
        "\n",
        "The value 0.5 here is also an example and indicates a percentage or relative value of these characteristics. If key features detected in the image are below this threshold, the image is not considered a slide.\n",
        "This parameter can also be adapted based on the specific characteristics of your video slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "grjM8uJlf_U1"
      },
      "outputs": [],
      "source": [
        "# Slides detection with no-slide-sharing error prevention\n",
        "\n",
        "# Checks if the current image has characteristics that suggest it is a slide\n",
        "def is_slide(image, text_threshold=100, feature_threshold=0.5):\n",
        "    # Convert the image to gray scale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect text areas (you could use more advanced text detection methods)\n",
        "    # For now, let's use a simple thresholding method\n",
        "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
        "    text_area = cv2.countNonZero(thresh)\n",
        "\n",
        "    # Check if the amount of text is above a certain threshold\n",
        "    if text_area < text_threshold:\n",
        "        return False\n",
        "\n",
        "    # Check for key features (like specific shapes, colors, etc.)\n",
        "    # For now, this is just a placeholder for your specific feature detection logic\n",
        "    features = find_key_features(image)\n",
        "    if len(features) < feature_threshold:\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Modify the detect_slides function to use this new logic\n",
        "def detect_slides(video_path, threshold=0.7, skip_frames=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    ret, prev_frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to read the video\")\n",
        "        return []\n",
        "\n",
        "    prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame_count = 0\n",
        "    slide_changes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, curr_frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % skip_frames == 0:\n",
        "            if compare_slides(prev_frame, curr_frame, threshold) and is_slide(curr_frame):\n",
        "                timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "                timestamp_readable = milliseconds_to_hh_mm_ss(timestamp_ms)\n",
        "                slide_changes.append(timestamp_readable)\n",
        "                save_frame(curr_frame, timestamp_readable)  # Save the current slide\n",
        "                prev_frame = curr_frame\n",
        "                print(f\"Slide change detected at: {timestamp_readable}\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return slide_changes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHV66XRRqPvJ"
      },
      "source": [
        "###CHECKING FOR TEACHER NOTES ON SLIDES OR BLACK-BOARD (STILL TO BE TESTED!!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyE8NsINqG5w"
      },
      "outputs": [],
      "source": [
        "def frame_difference(frame1, frame2):\n",
        "    # Calcola la differenza assoluta tra due frame\n",
        "    diff = cv2.absdiff(frame1, frame2)\n",
        "    non_zero_count = np.count_nonzero(diff)\n",
        "    return non_zero_count\n",
        "\n",
        "def detect_changes(video_path, change_threshold=500000, skip_frames=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    ret, prev_frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to read the video\")\n",
        "        return []\n",
        "\n",
        "    prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame_count = 0\n",
        "    significant_changes = []\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, curr_frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % skip_frames == 0:\n",
        "            gray_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "            if frame_difference(prev_frame, gray_frame) > change_threshold:\n",
        "                timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "                timestamp_readable = milliseconds_to_hh_mm_ss(timestamp_ms)\n",
        "                significant_changes.append(timestamp_readable)\n",
        "                save_frame(curr_frame, timestamp_readable)  # Salva l'immagine corrente\n",
        "                prev_frame = gray_frame\n",
        "                print(f\"Change detected at: {timestamp_readable}\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return significant_changes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6wdvA8uqE7J"
      },
      "outputs": [],
      "source": [
        "def detect_slides_and_changes(video_path, slide_threshold=0.7, change_threshold=500000, skip_frames=30):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    ret, prev_frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to read the video\")\n",
        "        return []\n",
        "\n",
        "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    frame_count = 0\n",
        "    events = []  # Questa lista conterrà sia i cambi di slide sia le modifiche significative\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, curr_frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % skip_frames == 0:\n",
        "            curr_frame_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Controlla sia per i cambi di slide sia per le modifiche significative\n",
        "            if compare_slides(prev_frame_gray, curr_frame_gray, slide_threshold) or frame_difference(prev_frame_gray, curr_frame_gray) > change_threshold:\n",
        "                timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "                timestamp_readable = milliseconds_to_hh_mm_ss(timestamp_ms)\n",
        "                events.append({'type': 'change', 'timestamp': timestamp_readable})\n",
        "                save_frame(curr_frame, timestamp_readable)\n",
        "                print(f\"Event detected at: {timestamp_readable}\")\n",
        "                prev_frame_gray = curr_frame_gray\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return events\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUOVB_BJ84WY"
      },
      "source": [
        "63"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyUQrIWK1Tpc"
      },
      "outputs": [],
      "source": [
        "print(f\"detected slides {len(slide_changes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNAVDti0a1Cj"
      },
      "source": [
        "# Audio extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tHPDl3Wa36U",
        "outputId": "802f18b2-832e-4a3a-95fa-6df555af80a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.8.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from cohere)\n",
            "  Downloading boto3-1.35.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx>=0.21.2 (from cohere)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.8.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.20.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.19.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Collecting botocore<1.36.0,>=1.35.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading botocore-1.35.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.21.2->cohere)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.21.2->cohere)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.7)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.23.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.1->boto3<2.0.0,>=1.34.0->cohere) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.5)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.1->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
            "Downloading cohere-5.8.1-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.8/207.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading boto3-1.35.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastavro-1.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.35.1-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: types-requests, parameterized, jmespath, httpx-sse, h11, fastavro, httpcore, botocore, s3transfer, httpx, boto3, cohere\n",
            "Successfully installed boto3-1.35.1 botocore-1.35.1 cohere-5.8.1 fastavro-1.9.5 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 s3transfer-0.10.2 types-requests-2.32.0.20240712\n",
            "Collecting openai\n",
            "  Downloading openai-1.41.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.41.1-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.5/362.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, openai\n",
            "Successfully installed jiter-0.5.0 openai-1.41.1\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-rgpj0e4h\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-rgpj0e4h\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.5)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.3.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.15.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802824 sha256=4a6731ff0714682227f480d213d0b91f37d3dcaa50eee47f1aa0adcbbc18cabf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7w2pwff1/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere\n",
        "!pip install openai\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fIvPrh4a8KU"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "#if transcription suck change the model size: tiny < base < small < medium < large\n",
        "#if transcribing from english use .en eg small.en\n",
        "def transcribe_video(video_path, model_size='medium.en'):\n",
        "    model = whisper.load_model(model_size)\n",
        "    result = model.transcribe(video_path)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cw0oBthdbBJo"
      },
      "outputs": [],
      "source": [
        "def interleave_transcription_with_slides(transcription, slide_changes):\n",
        "    interleaved_data = []\n",
        "    current_slide = 0\n",
        "\n",
        "    for segment in transcription['segments']:\n",
        "        # Find the next slide change that occurs after this segment starts\n",
        "        while current_slide < len(slide_changes) and slide_changes[current_slide] <= segment['start']:\n",
        "            interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "            current_slide += 1\n",
        "\n",
        "        # Add the transcribed text\n",
        "        interleaved_data.append({'type': 'transcription', 'timestamp': segment['start'], 'text': segment['text']})\n",
        "\n",
        "    # Handle any remaining slide changes\n",
        "    while current_slide < len(slide_changes):\n",
        "        interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "        current_slide += 1\n",
        "\n",
        "    return interleaved_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKg6wdG6b7yV"
      },
      "source": [
        "##NEW OPTIMIZED FUNCTION BY LessonExtractorAI (STILL TO BE TESTED!!)\n",
        "To better synchronize slides with audio, it's important to ensure that transcription timing and slide change timestamps are accurate and aligned. One way to do this is to make sure the timestamps are consistent with each other and accurately reflect when a slide appears in the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJ_gTQnOb6qG"
      },
      "outputs": [],
      "source": [
        "def interleave_transcription_with_slides(transcription, slide_changes):\n",
        "    interleaved_data = []\n",
        "    current_slide = 0\n",
        "\n",
        "    for segment in transcription['segments']:\n",
        "        # Convert segment start time to a comparable format\n",
        "        segment_start_seconds = convert_timestamp_to_seconds(segment['start'])\n",
        "\n",
        "        # Find the next slide change that occurs after this segment starts\n",
        "        while current_slide < len(slide_changes) and slide_changes[current_slide] <= segment_start_seconds:\n",
        "            interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "            current_slide += 1\n",
        "\n",
        "        interleaved_data.append({'type': 'transcription', 'timestamp': segment['start'], 'text': segment['text']})\n",
        "\n",
        "    # Handle any remaining slide changes\n",
        "    while current_slide < len(slide_changes):\n",
        "        interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "        current_slide += 1\n",
        "\n",
        "    return interleaved_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqXpP6KDbPIc"
      },
      "source": [
        "#pdf or docx creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMh1C3O0bjHn",
        "outputId": "18ca98ea-6d8b-4beb-ef0e-89634d0be4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1sK46IiboN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b66141-1810-40b8-be8b-cb93263ea170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:30<00:00, 49.4MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document saved as: gpu-20240515_converted.docx\n"
          ]
        }
      ],
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt, Inches, Cm\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "from docx.enum.section import WD_SECTION_START\n",
        "from docx import Document\n",
        "from docx.shared import Pt, Inches\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "\n",
        "def create_document(interleaved_data, video_path, slides_folder='slides'):\n",
        "    # Crea il documento Word\n",
        "    doc = Document()\n",
        "    doc.styles['Normal'].font.name = 'Calibri'\n",
        "    doc.styles['Normal'].font.size = Pt(11)\n",
        "\n",
        "    for item in interleaved_data:\n",
        "        if item['type'] == 'slide_change':\n",
        "            # Aggiungi l'immagine della slide\n",
        "            slide_image_path = f\"{slides_folder}/slide_{item['timestamp']}.jpg\"\n",
        "            p = doc.add_paragraph()\n",
        "            p.alignment = WD_ALIGN_PARAGRAPH.CENTER  # Centra il paragrafo\n",
        "            try:\n",
        "                run = p.add_run()\n",
        "                run.add_picture(slide_image_path, width=Inches(6))  # Regola la larghezza se necessario\n",
        "                run.add_break()  # Aggiungi una pausa dopo l'immagine\n",
        "            except Exception as e:\n",
        "                p.add_run(f\"Could not load slide image: {slide_image_path}\")\n",
        "            p.add_run().add_break()  # Aggiungi una pausa prima dell'immagine successiva\n",
        "        else:\n",
        "            # Aggiungi il testo trascritto\n",
        "            cleaned_text = item['text'].strip().replace('\\n', ' ')\n",
        "            p = doc.add_paragraph(cleaned_text)\n",
        "            p.alignment = WD_ALIGN_PARAGRAPH.JUSTIFY  # Giustifica il testo\n",
        "\n",
        "    # Salva il documento\n",
        "    output_file = f\"{os.path.splitext(os.path.basename(video_path))[0]}.docx\"\n",
        "    doc.save(output_file)\n",
        "    print(f\"Document saved as: {output_file}\")\n",
        "\n",
        "# Altre funzioni...\n",
        "\n",
        "\n",
        "\n",
        "def convert_timestamp_to_seconds(timestamp_str):\n",
        "    parts = timestamp_str.split(':')\n",
        "    if len(parts) == 3:\n",
        "        hours, minutes, seconds = map(int, parts)\n",
        "        return hours * 3600 + minutes * 60 + seconds\n",
        "    elif len(parts) == 2:\n",
        "        minutes, seconds = map(int, parts)\n",
        "        return minutes * 60 + seconds\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid timestamp format: {timestamp_str}\")\n",
        "\n",
        "def interleave_transcription_with_slides(transcription, slide_changes):\n",
        "    # Convert slide change timestamps to seconds\n",
        "    slide_changes_seconds = [convert_timestamp_to_seconds(ts) for ts in slide_changes]\n",
        "\n",
        "    interleaved_data = []\n",
        "    current_slide = 0\n",
        "\n",
        "    for segment in transcription['segments']:\n",
        "        # Find the next slide change that occurs after this segment starts\n",
        "        while current_slide < len(slide_changes_seconds) and slide_changes_seconds[current_slide] <= segment['start']:\n",
        "            interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "            current_slide += 1\n",
        "\n",
        "        # Add the transcribed text\n",
        "        interleaved_data.append({'type': 'transcription', 'timestamp': segment['start'], 'text': segment['text']})\n",
        "\n",
        "    # Handle any remaining slide changes\n",
        "    while current_slide < len(slide_changes_seconds):\n",
        "        interleaved_data.append({'type': 'slide_change', 'timestamp': slide_changes[current_slide]})\n",
        "        current_slide += 1\n",
        "\n",
        "    return interleaved_data\n",
        "\n",
        "# Transcribe video\n",
        "transcription = transcribe_video(input_video_path)\n",
        "\n",
        "# Interleave transcription with slide changes\n",
        "interleaved_data = interleave_transcription_with_slides(transcription, slide_changes)\n",
        "\n",
        "# Create DOCX document\n",
        "create_document(interleaved_data, output_video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bOAgSizs2nD",
        "outputId": "cf834ac7-ad5d-4fdc-9b1d-5007b1858d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 20240404_converted.docx       lez15.mp4\n",
            " gpu-20240430_converted.docx   lez3_converted.docx\n",
            " gpu-20240430.mp4              lez3_slides_only.docx\n",
            " gpu-20240515.mp4              lez6.mp4\n",
            " gpu-20240517.mp4              lez9.docx\n",
            " gpu-20240529.mp4             'Marcello Restelli_s Personal Room-20240312 1333-1_converted.docx'\n",
            " gpu-20240531.mp4             'Marcello Restelli_s Personal Room-20240326 1330-1_converted.docx'\n",
            " LessonsExtractor.ipynb        ML20240416_converted.docx\n",
            " lez13_converted.docx          ML20240430_converted.docx\n",
            " lez13.mp4                     ML20240502_converted.docx\n",
            " lez15_converted.docx          \u001b[0m\u001b[01;34mslides\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s87R8Ymqum4s"
      },
      "source": [
        "eliminate the slides images after having generated the docx output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2xchC3lt87G"
      },
      "outputs": [],
      "source": [
        "%rm -rf /content/drive/MyDrive/lessonsExtractor/slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBMmUvPzqCN7"
      },
      "source": [
        "code to put only slides in a docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KkKI3FY5pMvy",
        "outputId": "a83ac21e-a0da-4288-a3d4-25e83c094d47"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'docx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-db21ef67baef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_document_with_slides_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslide_changes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslides_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'slides'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "import os\n",
        "\n",
        "def create_document_with_slides_only(slide_changes, video_path, slides_folder='slides'):\n",
        "    # Extract base name and remove extension to form the DOCX file name\n",
        "    base_name = os.path.basename(video_path)\n",
        "    file_name_without_ext = os.path.splitext(base_name)[0]\n",
        "    output_file = f'{file_name_without_ext}_slides_only.docx'\n",
        "\n",
        "    doc = Document()\n",
        "\n",
        "    for timestamp in slide_changes:\n",
        "        slide_image_path = f\"{slides_folder}/slide_{timestamp}.jpg\"\n",
        "        p = doc.add_paragraph()\n",
        "        try:\n",
        "            p.add_run().add_picture(slide_image_path, width=Inches(6))  # Adjust the width as needed\n",
        "        except Exception as e:\n",
        "            p.add_run(f\"Could not load slide image: {slide_image_path}\")\n",
        "\n",
        "    doc.save(output_file)\n",
        "    print(f\"Document saved as: {output_file}\")\n",
        "\n",
        "# Example usage:\n",
        "# slide_changes = ['00:00:10', '00:01:15', '00:02:30']  # List of timestamps for slide changes\n",
        "create_document_with_slides_only(slide_changes, input_video_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DCgnJHZKZwh1",
        "kKg6wdG6b7yV"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}